{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import concurrent.futures\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Đọc file CSV\n",
    "list_file = \"all_link_list.csv\"\n",
    "df = pd.read_csv(list_file, encoding='utf-8')\n",
    "\n",
    "print(len(df))\n",
    "print(df.head(5))\n",
    "def detailinforequest(index, cntyCd, baseYy, reffNoNm, prlstClsfSrno):\n",
    "    # URL của API\n",
    "    url = 'https://unipass.customs.go.kr/clip/prlstclsfsrch/retrievePrlstClsfCaseDtl.do'\n",
    "\n",
    "    # Headers\n",
    "    headers = {\n",
    "        'Accept': 'text/html, */*; q=0.01',\n",
    "        'Accept-Language': 'en-US,en;q=0.9,vi;q=0.8,vi-VN;q=0.7',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',\n",
    "        'Cookie': 'WMONID=N_4A3dINH0I; JSESSIONID=0001O2y8S-yyQQ_lga-knblPV6Lb2QN6662ivRFJ_jK2FPRlLOTKNEsnuhZe_QxBEKq-8U03_kah8EgUgb-_Exa4dmevvhJHEKeNuIq9prRxpJht6ugFASENgz3p_AdMPw4_:eul21',\n",
    "        'DNT': '1',\n",
    "        'Origin': 'https://unipass.customs.go.kr',\n",
    "        'Referer': 'https://unipass.customs.go.kr/clip/index.do',\n",
    "        'Sec-Fetch-Dest': 'empty',\n",
    "        'Sec-Fetch-Mode': 'cors',\n",
    "        'Sec-Fetch-Site': 'same-origin',\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36',\n",
    "        'X-Requested-With': 'XMLHttpRequest',\n",
    "        'isAjax': 'true',\n",
    "        'sec-ch-ua': '\"Google Chrome\";v=\"129\", \"Not=A?Brand\";v=\"8\", \"Chromium\";v=\"129\"',\n",
    "        'sec-ch-ua-mobile': '?0',\n",
    "        'sec-ch-ua-platform': '\"Windows\"'\n",
    "    }\n",
    "\n",
    "    # Dữ liệu trong body của yêu cầu\n",
    "    payload = {\n",
    "        'cntyCd': cntyCd,\n",
    "        'baseYy': baseYy,\n",
    "        'reffNoNm': reffNoNm,\n",
    "        'prlstClsfSrno': prlstClsfSrno\n",
    "    }\n",
    "\n",
    "    # Gửi yêu cầu POST với headers và dữ liệu body\n",
    "    response = requests.post(url, headers=headers, data=payload,timeout=60)\n",
    "\n",
    "    # Kiểm tra phản hồi từ API\n",
    "    if response.status_code == 200:\n",
    "        return response.text, index\n",
    "    else:\n",
    "        print(f\"Row {index} Lỗi\")\n",
    "        return None, index\n",
    "\n",
    "\n",
    "def extract_data_from_html(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    result = {}\n",
    "    for table_type in ('org','eng','kor'):\n",
    "        table = soup.find(\"table\", class_=table_type)\n",
    "        # Tìm thẻ <p> chứa từ khóa \"상세결과 항목 :\"\n",
    "        all_ths = table.findAll('th')\n",
    "        for th in all_ths: \n",
    "            if th.get_text() == \"관련 이미지\":\n",
    "                image_elements = table.find('th', text='관련 이미지').find_next_sibling('td').find_all('img')\n",
    "                image_links = [img['src'] for img in image_elements]\n",
    "                result[f\"{th.get_text()}_{table_type}\"] = image_links\n",
    "            # print(f\"Đang xử lý: {catalog}\")\n",
    "            else: \n",
    "                elems_html_info = table.find('th', text=th.get_text()).find_next_sibling('td')\n",
    "                for elem in elems_html_info:\n",
    "                    if elem.name == 'a':  # Thay đổi thẻ <a> thành văn bản\n",
    "                        elem.replace_with(elem.text)\n",
    "                    elif elem.name == 'br':  # Giữ lại thẻ <br>\n",
    "                        continue\n",
    "                elem_text = elems_html_info.decode_contents().replace('\\r', '').replace('\\t', '').replace('\\n', '').strip()\n",
    "                result[f\"{th.get_text()}_{table_type}\"] = elem_text\n",
    "    print(result)\n",
    "    return result\n",
    "\n",
    "\n",
    "def save_state(states, thread_index):\n",
    "    with open(state_file_template.format(thread_index), \"w\") as f:\n",
    "        json.dump(states, f)\n",
    "\n",
    "\n",
    "def load_state(thread_index):\n",
    "    try:\n",
    "        with open(state_file_template.format(thread_index), \"r\") as f:\n",
    "            states = json.load(f)\n",
    "            if isinstance(states, dict) and \"start_index\" in states:\n",
    "                return states\n",
    "            else:\n",
    "                return {\"start_index\": 0}\n",
    "    except FileNotFoundError:\n",
    "        return {\"start_index\": 0}\n",
    "\n",
    "\n",
    "def process_data_chunk(data_chunk, thread_index, start_index):\n",
    "\n",
    "    all_detail_output = []\n",
    "    error_stt_list = []\n",
    "    current_index = start_index\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            for index, row in data_chunk.iloc[start_index:].iterrows():\n",
    "                cntyCd = row['cntyCd']\n",
    "                baseYy = row['baseYy']\n",
    "                reffNoNm = row['reffNoNm']\n",
    "                prlstClsfSrno = row['prlstClsfSrno']\n",
    "                stt = row['stt']\n",
    "\n",
    "                try:\n",
    "                    html_content, index = detailinforequest(index, cntyCd, baseYy, reffNoNm, prlstClsfSrno)\n",
    "                    if html_content:\n",
    "                        detail_output = extract_data_from_html(html_content)\n",
    "                        detail_output[\"stt\"] = stt\n",
    "                        all_detail_output.append(detail_output)\n",
    "\n",
    "                        # Ghi kết quả vào file riêng cho thread\n",
    "                        with open(output_file_template.format(thread_index), \"a\", encoding='utf-8') as f:\n",
    "                            f.write(json.dumps(detail_output, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing row {index}: {e}\")\n",
    "                    error_stt_list.append(stt)\n",
    "                    \n",
    "                    # Ghi lỗi vào file\n",
    "                    with open(error_file_template.format(thread_index), \"a\", encoding='utf-8') as f:\n",
    "                        f.write(json.dumps({\"stt\": stt, \"error\": str(e)}, ensure_ascii=False) + \"\\n\")\n",
    "                    \n",
    "                finally:\n",
    "                    # Cập nhật trạng thái đã xử lý\n",
    "                    current_index = index + 1  \n",
    "                    save_state({\"start_index\": current_index}, thread_index)\n",
    "\n",
    "            return all_detail_output, current_index, error_stt_list\n",
    "        except Exception as e:\n",
    "            print(f\"Thread {thread_index} encountered an error: {e}. Restarting...\")\n",
    "            time.sleep(5)  # Đợi 5 giây trước khi thử lại\n",
    "\n",
    "# Thư mục để lưu trạng thái và kết quả\n",
    "output_dir = \"org\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# File để lưu trạng thái của vòng lặp\n",
    "state_file_template = os.path.join(output_dir, \"loop_state_{}.json\")\n",
    "output_file_template = os.path.join(output_dir, \"output_data_{}.txt\")\n",
    "error_file_template = os.path.join(output_dir, \"error_stt_{}.txt\")\n",
    "final_output_file = \"final_output_data_org.txt\"  # File tổng kết quả\n",
    "\n",
    "# Bắt đầu vòng lặp từ trạng thái đã lưu\n",
    "chunk_size = 100000\n",
    "df_chunks = [chunk.reset_index(drop=True) for chunk in [df[i:i + chunk_size] for i in range(0, len(df), chunk_size)]]\n",
    "\n",
    "# Bắt đầu vòng lặp từ trạng thái đã lưu\n",
    "saved_states = [load_state(i) for i in range(len(df_chunks))]\n",
    "\n",
    "# Sử dụng multithreading để xử lý từng chunk\n",
    "# all_results = []\n",
    "# all_error_stts = []\n",
    "states = {}\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=len(df_chunks)) as executor:\n",
    "    futures = {\n",
    "        executor.submit(process_data_chunk, chunk, idx, saved_states[idx][\"start_index\"]): idx\n",
    "        for idx, chunk in enumerate(df_chunks)\n",
    "    }\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        thread_index = futures[future]\n",
    "        try:\n",
    "            results, current_index, error_stts = future.result()\n",
    "            # all_results.extend(results)\n",
    "            # all_error_stts.extend(error_stts)\n",
    "            states[thread_index] = {\"start_index\": current_index}\n",
    "        except Exception as e:\n",
    "            print(f\"Thread {thread_index} encountered an error: {e}\")\n",
    "            save_state(states[thread_index], thread_index)\n",
    "\n",
    "# # Ghi tất cả các lỗi vào file chung\n",
    "# with open(\"final_error_stts.txt\", \"w\", encoding='utf-8') as f:\n",
    "#     for error_stt in all_error_stts:\n",
    "#         f.write(f\"{error_stt}\\n\")\n",
    "\n",
    "# # Lưu kết quả cuối cùng\n",
    "# with open(final_output_file, \"w\", encoding='utf-8') as f:\n",
    "#     for result in all_results:\n",
    "#         f.write(json.dumps(result, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "# Thư mục chứa các file TXT\n",
    "directory = 'org/'  # Thay đổi thành đường dẫn thư mục chứa các file TXT của bạn\n",
    "\n",
    "# Tạo danh sách các file theo mẫu tên\n",
    "error_file_names = [f'error_stt_{i}.txt' for i in range(12)]\n",
    "\n",
    "# Khởi tạo danh sách để chứa dữ liệu từ tất cả các file\n",
    "all_error_data = []\n",
    "\n",
    "# Đọc và xử lý từng file\n",
    "for file_name in error_file_names:\n",
    "    file_path = os.path.join(directory, file_name)\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        # Đọc từng dòng và chuyển đổi từ JSON thành dictionary\n",
    "        data = [json.loads(line) for line in file]\n",
    "        all_error_data.extend(data)  # Thêm dữ liệu từ file hiện tại vào danh sách tổng\n",
    "\n",
    "# Chuyển tất cả dữ liệu thành DataFrame\n",
    "error_df = pd.DataFrame(all_error_data)\n",
    "\n",
    "# Lưu DataFrame ra file CSV nếu cần\n",
    "error_df.to_csv('all_error_stt_2.csv', index=False, encoding='utf-8')\n",
    "\n",
    "output_file_names = [f'output_data_{i}.txt' for i in range(12)]\n",
    "\n",
    "# Khởi tạo danh sách để chứa dữ liệu từ tất cả các file\n",
    "all_output_data = []\n",
    "\n",
    "# Đọc và xử lý từng file\n",
    "for file_name in output_file_names:\n",
    "    file_path = os.path.join(directory, file_name)\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        # Đọc từng dòng và chuyển đổi từ JSON thành dictionary\n",
    "        data = [json.loads(line) for line in file]\n",
    "        all_output_data.extend(data)  # Thêm dữ liệu từ file hiện tại vào danh sách tổng\n",
    "\n",
    "# Chuyển tất cả dữ liệu thành DataFrame\n",
    "output_df = pd.DataFrame(all_output_data)\n",
    "\n",
    "# Lưu DataFrame ra file CSV nếu cần\n",
    "output_df.to_csv('all_output_data_2.csv', index=False, encoding='utf-8')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
