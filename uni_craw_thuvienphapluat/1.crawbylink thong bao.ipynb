{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from lxml import etree\n",
    "from datetime import datetime\n",
    "import openpyxl\n",
    "from openpyxl import Workbook\n",
    "from openpyxl import load_workbook\n",
    "current_directory = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_date(text):\n",
    "    # Biểu thức chính quy để bắt ngày, tháng, năm\n",
    "    pattern = r\"(\\d{1,2})\\s*tháng\\s*(\\d{1,2})\\s*năm\\s*(\\d{4})\"\n",
    "    match = re.search(pattern, text)\n",
    "    \n",
    "    if match:\n",
    "        # Lấy ra các thành phần ngày, tháng, năm\n",
    "        day, month, year = match.groups()\n",
    "        \n",
    "        # Chuyển đổi thành định dạng dd-mm-yyyy\n",
    "        date_obj = datetime.strptime(f\"{day}-{month}-{year}\", \"%d-%m-%Y\")\n",
    "        return date_obj.strftime(\"%d-%m-%Y\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def part_text(part,start,end):\n",
    "    part_list = []\n",
    "    for i in part[start:end]:\n",
    "        part_list.append(i.get_text().replace('\\r', '').replace('\\n', '').strip())\n",
    "        part_text = '\\n'.join(part_list)\n",
    "    return part_text\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phanloai_link_info(link):\n",
    "    response2 = requests.get(link.strip())\n",
    "    soup = BeautifulSoup(response2.text, 'html.parser')\n",
    "    content1 = soup.find('div', class_='content1')\n",
    "    tables = content1.find_all('table')\n",
    "\n",
    "    # Khởi tạo các biến trước để tránh lỗi khi không tìm thấy\n",
    "    số = \"Không tìm thấy 'Số'\"\n",
    "    ngày = \"Không tìm thấy 'Ngày'\"\n",
    "    part_1_text = \"Không tìm thấy part 1\"\n",
    "    part_2_text = \"Không tìm thấy part 2\"\n",
    "    part_3_text = \"Không tìm thấy part 3\"\n",
    "    part_4_text = \"Không tìm thấy part 4\"\n",
    "    part_5_text = \"Không tìm thấy thông tin phần 5\"\n",
    "    part_6_text = \"Không tìm thấy thông tin phần 6\" # Tổ chức thực thi\n",
    "\n",
    "    # Lấy số văn bản / ngày\n",
    "    try:\n",
    "        paragraphs = tables[0].find_all('p')  # Tìm tất cả các thẻ <p> trong bảng đầu tiên\n",
    "        for para in paragraphs:\n",
    "            text = para.get_text(strip=True).replace('\\r', '').replace('\\n', '').strip()\n",
    "            if 'Số:' in text:\n",
    "                số = text.replace(\"Số:\", \"\").strip()\n",
    "                break  # Ngừng vòng lặp sau khi tìm thấy đoạn văn bản chứa 'Số'\n",
    "    except AttributeError as e:\n",
    "        print(f\"Lỗi khi tìm 'Số': {e}\")\n",
    "\n",
    "    try:\n",
    "        paragraphs = tables[0].find_all('p')  # Tìm tất cả các thẻ <p> trong bảng đầu tiên\n",
    "        for para in paragraphs:\n",
    "            text = para.get_text(strip=True).replace('\\r', '').replace('\\n', '').strip()\n",
    "            if 'ngày' in text or 'Ngày' in text:\n",
    "                ngày = normalize_date(text)\n",
    "                break  # Ngừng vòng lặp sau khi tìm thấy đoạn văn bản chứa 'ngày'\n",
    "    except AttributeError as e:\n",
    "        print(f\"Lỗi khi tìm 'ngày': {e}\")\n",
    "\n",
    "    # Lấy part 1 - bảng đầu của trang\n",
    "    part1 = tables[1].find_all('p')       \n",
    "    part1_1_pattern = re.compile(r\"1\\s*\\.\\s*[Tt]ên\\s*[Hh]àng\")\n",
    "    part1_2_pattern = re.compile(r\"2\\s*\\.\\s*Đơn\\s*vị\")\n",
    "    part1_3_pattern = re.compile(r\"3\\s*\\.\\s*Số\")\n",
    "    part1_3_pattern_2 = re.compile(r\"3\\s*\\.\\s*Tờ\\s*khai\")\n",
    "    part1_4_pattern = re.compile(r\"4\\s*\\.\\s*Tóm\\s*tắt\")\n",
    "    part1_5_pattern = re.compile(r\"5\\s*\\.\\s*Kết\\s*quả\")\n",
    "\n",
    "    # Tìm index các phần\n",
    "    part1_1_index = next((i for i, p_tag in enumerate(part1) if part1_1_pattern.search(p_tag.get_text(strip=True))), None)\n",
    "    part1_2_index = next((i for i, p_tag in enumerate(part1) if part1_2_pattern.search(p_tag.get_text(strip=True))), None)\n",
    "    part1_3_index = next((i for i, p_tag in enumerate(part1) if part1_3_pattern.search(p_tag.get_text(strip=True)) or part1_3_pattern_2.search(p_tag.get_text(strip=True))), None)\n",
    "    part1_4_index = next((i for i, p_tag in enumerate(part1) if part1_4_pattern.search(p_tag.get_text(strip=True))), None)\n",
    "    part1_5_index = next((i for i, p_tag in enumerate(part1) if part1_5_pattern.search(p_tag.get_text(strip=True))), None)\n",
    "\n",
    "    # Kiểm tra nếu các chỉ mục có tồn tại, nếu không gán giá trị thông báo không tìm thấy\n",
    "    if part1_1_index is not None and part1_2_index is not None:\n",
    "        part_1_text = part_text(part1, part1_1_index, part1_2_index)\n",
    "    if part1_2_index is not None and part1_3_index is not None:\n",
    "        part_2_text = part_text(part1, part1_2_index, part1_3_index)\n",
    "    if part1_3_index is not None and part1_4_index is not None:\n",
    "        part_3_text = part_text(part1, part1_3_index, part1_4_index)\n",
    "    if part1_4_index is not None and part1_5_index is not None:\n",
    "        part_4_text = part_text(part1, part1_4_index, part1_5_index)\n",
    "\n",
    "    # Lấy thông tin phần 5 nếu tìm thấy index\n",
    "    if part1_5_index is not None:\n",
    "        pattern = re.compile(r'\\d{4}\\s*\\.\\s*\\d{2}\\s*\\.\\s*\\d{2}')\n",
    "        for p_tag in part1[part1_5_index:]:\n",
    "            text = p_tag.get_text(strip=True)  # Lấy văn bản từ thẻ <p> và loại bỏ khoảng trắng thừa\n",
    "            matches = pattern.findall(text)  # Tìm tất cả các dãy số khớp với pattern trong văn bản\n",
    "            if matches:\n",
    "                part_5_text = matches[-1]  # Lấy kết quả cuối cùng khớp với pattern\n",
    "                part_5_full = p_tag.get_text().replace('\\r', '').replace('\\n', '').strip()\n",
    "                \n",
    "    # Lấy thông tin phần 6\n",
    "    for sibling in tables[1].previous_siblings:\n",
    "        if sibling.name == 'p':\n",
    "            p_before_table1 = sibling\n",
    "            break\n",
    "    # In ra thẻ <p> nếu tìm thấy\n",
    "    if p_before_table1:\n",
    "        part_6_text = p_before_table1.get_text().replace('\\r', ' ').replace('\\n', ' ').replace('  ', ' ').strip()\n",
    "\n",
    "    # Tạo danh sách chứa thông tin\n",
    "    info_list = [link.strip(), số, ngày, part_1_text, part_2_text, part_3_text, part_4_text, part_5_text,part_5_full,part_6_text]\n",
    "    info_list = [re.sub(r'[ ]+', ' ', text).strip() for text in info_list]\n",
    "\n",
    "    print(link)\n",
    "    print(info_list)\n",
    "    return info_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('links_for_craw.txt', 'r', encoding='utf-8') as file:\n",
    "    all_links = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_phanloai_info_list=[]\n",
    "for link in all_links:\n",
    "    if 'phan-loai' in link:\n",
    "        try:\n",
    "            info_list = get_phanloai_link_info(link)\n",
    "            all_phanloai_info_list.append(info_list)\n",
    "            # write_data_to_excel(phanloai_sheet,info_list)\n",
    "        except:\n",
    "            print(link + \"lỗi\")\n",
    "            info_list = [link,\"lỗi\"]\n",
    "            all_phanloai_info_list.append(info_list)\n",
    "            # write_data_to_excel(phanloai_sheet,info_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(all_phanloai_info_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"phan_loai_18022025.xlsx\",index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
